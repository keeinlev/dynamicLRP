% Share this document for others to view with this URL
% https://www.overleaf.com/read/cvfmtqkfphmr#384bf0
%
\documentclass{article}

\input{preamble}
\input{math_commands}

\usepackage{natbib}
\usepackage{etoolbox}       % Provides toggle
\newtoggle{arxiv}% Swap between arXiv and journal/conference version
\toggletrue{arxiv}% Enable arXiv version --- change this to swap between arXiv and journal/conference
\iftoggle{arxiv}{
  % Things just for the arXiv version go here
  \usepackage[parfill]{parskip}

  \usepackage[%
    a4paper,
    inner=25mm,
    outer=25mm,% = marginparsep + marginparwidth + 5mm (between marginpar and page border)
    top=25mm,
    bottom=25mm,
    marginparsep=5mm,
    marginparwidth=40mm,
    %showframe% to show your page design, normally not used
  ]{geometry}

  \usepackage{tikz}
  \usetikzlibrary{positioning,arrows.meta}

  \tikzset{
  maincircle/.style={circle, draw, minimum size=13mm, inner sep=2pt, font=\footnotesize, align=center},
  fwd/.style={->, thick, >=Latex},
  promise/.style={->, dashed, thick, >=Latex,
                  preaction={draw, white, line width=3pt}}, % halo for visibility
}
  \newlength{\defbaselineskip}
  \setlength{\defbaselineskip}{\baselineskip}
  % \setlength{\marginparwidth}{0.8in}
  \setlength{\parskip}{6pt}%
  \setlength{\parindent}{0pt}%

  % If you find the font is not Libertine, move this block lower down the preamble, to just before \begin{document}
  % If other packages later also import fonts, that will override the fonts introduced here.
  \RequirePackage[T1]{fontenc}
  \RequirePackage[tt=false, type1=true]{libertine}
  \RequirePackage[varqu]{zi4}
  \RequirePackage[libertine]{newtxmath}

  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \fancyhead[L]{PAPER SHORTNAME}
  \fancyhead[R]{FIRST AUTHOR, et al. (YEAR)}

  \usepackage{authblk}% alternative author layout that supports symbols to indicate affiliations
  \renewcommand*{\Affilfont}{\normalsize}
}{
  % Things just for the journal/conference version go here
}

\title{Make sure to always keep your promises: A model-agnostic attribution algorithm for Neural Networks}

\author[1]{FIRST AUTHOR NAME}
\affil[1]{FIRST AFFILIATION}
\affil[ ]{%
\texttt{AUTHOR1@EMAIL.ADDRESS}\quad
\texttt{AUTHOR2@EMAIL.ADDRESS}
}

\date{}% Suppress date


\begin{document}

\maketitle

\begin{abstract}
TBC
\end{abstract}

\section{Introduction}

Deep learning has fundamentally transformed machine learning by achieving unprecedented performance across vision, language, and multimodal tasks. However, this success comes at the cost of interpretability. While significant progress has been made in explaining medium-scale models and established architectures, the decision-making processes of large-scale transformer systems \cite{vaswani2017attention} remain challenging to interpret comprehensively. As these models increasingly influence high-stakes applications from healthcare to autonomous systems, understanding \emph{why} they make specific predictions has become a critical scientific and societal imperative.

Interpretability algorithms have emerged to address this challenge, offering diverse attribution methods that aim to identify which input features drive model predictions. These approaches broadly fall into three paradigms: gradient-based methods that leverage backpropagated derivatives \cite{simonyan2014deep}, perturbation-based techniques that measure prediction sensitivity to input modifications \cite{lundberg2017unified}, and rule-based decomposition methods that analytically redistribute output scores to input features. While gradient methods suffer from noise and saturation effects, and perturbation approaches incur prohibitive computational costs for large models, rule-based attribution offers a compelling middle ground, providing principled, efficient explanations through customizable propagation rules.

Layer-wise Relevance Propagation (LRP) \cite{bach2015pixel} stands as the most prominent rule-based attribution method, grounded in Deep Taylor Decomposition theory \cite{montavon2017explaining}. LRP's core insight is elegant: systematically decompose a neural network's output by backpropagating relevance scores through layers while preserving conservation properties. This approach has proven remarkably effective for convolutional networks and demonstrated theoretical advantages over gradient-based alternatives. However, LRP has struggled to keep pace with architectural innovations, particularly the attention mechanisms and normalization layers that define modern transformers \cite{ding2017saliency, voita2021analyzing}.

Recent advances have begun addressing these technical barriers. Achtibat et al. \cite{achtibat2024attnlrp} developed specialized propagation rules for attention layers, demonstrating LRP's viability for transformer architectures. Yet despite these developments, LRP remains systematically underrepresented in contemporary explainability research. This oversight is exemplified by Machiraju et al.'s \cite{machiraju2024prospector} comprehensive attribution study, which evaluated gradients, attention maps, SHAP, and LIME across transformer models while entirely omitting LRP—despite its proven effectiveness and computational advantages. Such systematic neglect reflects LRP's broader marginalization in the modern machine learning landscape.

We address this gap by proposing a model-agnostic LRP framework that extends rule-based attribution to arbitrary neural architectures with minimal overhead. Our key insight is decomposing LRP rules to individual tensor operations—analogous to how PyTorch's autograd generalized gradient computation—enabling seamless integration with any network topology. Through a novel promise-based system for handling missing forward activations, our approach maintains LRP's theoretical guarantees while achieving computational efficiency comparable to standard backpropagation. This work repositions LRP as a competitive, scalable alternative for explaining modern deep learning systems.

\section{Background}

Neural network interpretability algorithms fundamentally seek to decompose model predictions into input feature contributions, yet differ dramatically in their mathematical foundations and computational approaches. Gradient-based methods leverage the chain rule to compute $\frac{\partial f}{\partial x}$ as attribution signals \cite{simonyan2014deep, sundararajan2017axiomatic, smilkov2017smoothgrad}, but suffer from gradient saturation and linearity assumptions that fail to capture complex nonlinear interactions \cite{ancona2018towards}. Perturbation-based approaches like SHAP \cite{lundberg2017unified} and LIME \cite{ribeiro2016should} directly measure prediction sensitivity through game-theoretic frameworks or local surrogates, yet incur exponential computational costs, making them intractable for high-dimensional data \cite{covert2020understanding}. Class Activation Mapping methods \cite{zhou2016learning, selvaraju2017grad} leverage architectural constraints for spatial localization but provide coarse-grained explanations lacking principled mathematical foundations for complex architectures.

Rule-based decomposition methods, pioneered by Layer-wise Relevance Propagation (LRP) \cite{bach2015pixel}, offer a compelling alternative through systematic relevance redistribution. Unlike gradients, which reflect local sensitivity, LRP decomposes the actual prediction by backpropagating relevance scores while preserving conservation properties. This approach enables customizable propagation rules tailored to specific architectural components \cite{montavon2017explaining}. Notably, this modular design has historically required manual rule specification for each operation type, limiting its applicability to rapidly evolving architectures like transformers, where novel operations like multi-head attention and layer normalization demanded specialized treatment \cite{achtibat2024attnlrp}. The next section formally introduces classic LRP via Taylor approximation as it lays the groundwork for our proposed model-agnostic extension.

\subsection{Classic LRP via Taylor Approximation}

Classic LRP redistributes the prediction score (relevance) backwards through the layers of a neural network. The key assumption is that the relevance at output neuron $j$ is proportional to the function value at $j$: $R_j \propto f_j(x)$. To propagate relevance to the inputs, we use the Taylor expansion and the conservation rule.

For a function $f_j(x)$, the first-order Taylor expansion around a root point $x_0$ is:
\begin{equation}
f_j(x) \approx f_j(x_0) + \sum_i \frac{\partial f_j}{\partial x_i}\Big|_{x_0} (x_i - x_{0,i})
\end{equation}
The conservation rule requires that the sum of input relevances equals the output relevance:
\begin{equation}
\sum_i R_{i \leftarrow j} = R_j
\end{equation}
Assuming $R_{i \leftarrow j}$ is proportional to the Taylor term for input $i$:
\begin{equation}
R_{i \leftarrow j} = \frac{\frac{\partial f_j}{\partial x_i} (x_i - x_{0,i})}{f_j(x) - f_j(x_0) + \epsilon} R_j
\end{equation}
For a linear layer $y_j = \sum_i x_i w_{ji} + b_j$, and choosing $x_0 = 0$, this simplifies to:
\begin{equation}
R_{i \leftarrow j} = \frac{x_i w_{ji}}{y_j + \epsilon} R_j
\end{equation}
where $\epsilon$ is a small stabilizer to avoid division by zero. The total relevance assigned to input $i$ is then $R_i = \sum_j R_{i \leftarrow j}$. This derivation shows how the epsilon rule arises from the proportionality assumption, Taylor expansion, and conservation principle, linking the mathematical definition of LRP to its practical implementation.

\subsection{Computation Graphs in Deep Learning Frameworks}

Deep learning frameworks rely on computation graphs to represent the sequence of operations performed during the forward and backward passes of a neural network. Formally, a computation graph is a directed acyclic graph (DAG) $G = (V, E)$, where $V$ is the set of nodes representing operations (e.g., matrix multiplication, activation functions), and $E$ is the set of directed edges representing data flow between operations.

The backward pass in deep learning leverages the chain rule of calculus to compute gradients efficiently. For a scalar loss function $L$ and a parameter $\theta$, the gradient $\frac{\partial L}{\partial \theta}$ is computed by traversing the graph in reverse order:
\begin{equation}
\frac{\partial L}{\partial \theta} = \sum_{v \in \text{children}(\theta)} \frac{\partial L}{\partial f_v} \cdot \frac{\partial f_v}{\partial \theta},
\end{equation}
where $\text{children}(\theta)$ are the nodes that depend on $\theta$.

Our promise-based attribution algorithm extends this concept by associating each node in the graph with a custom propagation rule for relevance. This modular approach allows for operation-specific attributions, enabling the algorithm to handle complex architectures like transformers while adhering to the conservation principle:
\begin{equation}
\sum_{i \in \text{inputs}} R_i = \sum_{j \in \text{outputs}} R_j,
\end{equation}
where $R_i$ and $R_j$ are the relevance scores at the input and output nodes, respectively.

\paragraph{Background: Classic LRP via Taylor Approximation}


Classic LRP redistributes the prediction score (relevance) backwards through the layers of a neural network. The key assumption is that the relevance at output neuron $j$ is proportional to the function value at $j$: $R_j \propto f_j(x)$. To propagate relevance to the inputs, we use the Taylor expansion and the conservation rule.

For a function $f_j(x)$, the first-order Taylor expansion around a root point $x_0$ is:
\begin{equation}
f_j(x) \approx f_j(x_0) + \sum_i \frac{\partial f_j}{\partial x_i}\Big|_{x_0} (x_i - x_{0,i})
\end{equation}
The conservation rule requires that the sum of input relevances equals the output relevance:
\begin{equation}
\sum_i R_{i \leftarrow j} = R_j
\end{equation}
Assuming $R_{i \leftarrow j}$ is proportional to the Taylor term for input $i$:
\begin{equation}
R_{i \leftarrow j} = \frac{\frac{\partial f_j}{\partial x_i} (x_i - x_{0,i})}{f_j(x) - f_j(x_0) + \epsilon} R_j
\end{equation}
For a linear layer $y_j = \sum_i x_i w_{ji} + b_j$, and choosing $x_0 = 0$, this simplifies to:
\begin{equation}
R_{i \leftarrow j} = \frac{x_i w_{ji}}{y_j + \epsilon} R_j
\end{equation}
where $\epsilon$ is a small stabilizer to avoid division by zero. The total relevance assigned to input $i$ is then $R_i = \sum_j R_{i \leftarrow j}$. This derivation shows how the epsilon rule arises from the proportionality assumption, Taylor expansion, and conservation principle, linking the mathematical definition of LRP to its practical implementation.


Our work builds on these insights to address the limitations of existing methods, providing a scalable and efficient solution for interpreting transformer models.

\subsection{Computation Graphs in Deep Learning Frameworks}

Deep learning frameworks rely on computation graphs to represent the sequence of operations performed during the forward and backward passes of a neural network. Formally, a computation graph is a directed acyclic graph (DAG) $G = (V, E)$, where $V$ is the set of nodes representing operations (e.g., matrix multiplication, activation functions), and $E$ is the set of directed edges representing data flow between operations.

Each node $v \in V$ in the graph corresponds to a function $f_v$ that maps its inputs to outputs. For example, a linear layer can be represented as $f_v(x) = Wx + b$, where $W$ and $b$ are the weights and biases, respectively. The edges $e \in E$ capture the dependencies between these operations, ensuring that the graph is acyclic and can be traversed in topological order.

The backward pass in deep learning leverages the chain rule of calculus to compute gradients efficiently. For a scalar loss function $L$ and a parameter $\theta$, the gradient $\frac{\partial L}{\partial \theta}$ is computed by traversing the graph in reverse order:
\begin{equation}
\frac{\partial L}{\partial \theta} = \sum_{v \in \text{children}(\theta)} \frac{\partial L}{\partial f_v} \cdot \frac{\partial f_v}{\partial \theta},
\end{equation}
where $\text{children}(\theta)$ are the nodes that depend on $\theta$.

The promise-based attribution algorithm extends this concept by associating each node in the graph with a custom propagation rule for relevance. This modular approach allows for operation-specific attributions, enabling the algorithm to handle complex architectures like transformers. By leveraging the DAG structure, the algorithm ensures that relevance is propagated efficiently and faithfully, adhering to the conservation principle:
\begin{equation}
\sum_{i \in \text{inputs}} R_i = \sum_{j \in \text{outputs}} R_j,
\end{equation}
where $R_i$ and $R_j$ are the relevance scores at the input and output nodes, respectively.

This theoretical foundation underscores the flexibility and scalability of the promise-based approach, making it well-suited for modern deep learning models.

\section{Methods}

\subsection{Computation Graph Processing}

The computation graph is a directed acyclic graph (DAG) representing the sequence of operations in a neural network. However, the autograd graph only has one entrypoint at the Node representing the model output, and tracks only the backward edges from output to input. Algorithm~\ref{alg:graph-construction} outlines the process of constructing an auxiliary graph from the autograd graph of a PyTorch model. This graph will allow access to any Node's in- and out-adjacencies, and will be used in the traversal algorithm.

\begin{algorithm}[t]
  \caption{Computation Graph Construction}
  \label{alg:graph-construction}
  \begin{algorithmic}[1]
    \Require Model output $hidden\_states$
    \Ensure In-adjacency list $in\_adj$, Out-adjacency list $out\_adj$, Topologically sorted nodes $topo\_stack$
    \State Initialize $in\_adj \gets \emptyset$, $out\_adj \gets \emptyset$
    \State Initialize $visited \gets \emptyset$, $topo\_stack \gets \emptyset$
    \State $root \gets hidden\_states.grad\_fn$
    \State \Call{DFS}{$root$, $in\_adj$, $out\_adj$, $visited$, $topo\_stack$}
    \Return $in\_adj$, $out\_adj$, $topo\_stack$
    \Function{DFS}{$fcn$, $in\_adj$, $out\_adj$, $visited$, $topo\_stack$}
      \If{$fcn = \text{None}$ or $fcn \in visited$}
        \State \Return
      \EndIf
      \State $visited \gets visited \cup \{fcn\}$
      \For{each $child \in fcn.next\_functions$}
        \State $out\_adj[fcn].append(child)$
        \State $in\_adj[child].append(fcn)$
        \State \Call{DFS}{$child$, $in\_adj$, $out\_adj$, $visited$, $topo\_stack$}
      \EndFor
      \State $topo\_stack.push(fcn)$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection{Operation-Level Relevance Propagation}

We implement propagation rules for each type of autograd Node (a Node in this case maps to an operation call), then traverse the auxiliary graph in a style similar to Kahn's Algorithm (link to Appendix), but using a Depth-First heuristic instead of Breadth-First.
The operation-level approach modularizes relevance propagation, reducing any model architecture to a fixed set of fundamental tensor operations. Algorithm~\ref{alg:operation-propagation} describes the propagation process.

\begin{algorithm}[H]
  \caption{Operation-Level Relevance Propagation}
  \label{alg:operation-propagation}
  \begin{algorithmic}[1]
    \Require Model output $hidden\_states$, Relevance interpretation target $target\_node$, Auxiliary graph $in\_adj$, $out\_adj$, Output relevance $R_{out}$
    \Ensure Input relevance $R_{in}$
    \State Initialize $stack \gets [hidden\_states.grad\_fn]$
    \State Initialize $nodes\_pending \gets \{ node : len(parents)$ for $node, parents \in in\_adj.items() \}$
    \State Initialize $node\_inputs \gets \{ node : [ null$ for $parent$ in $parents$ ] for $node, parents \in in\_adj.items() \}$
    \State Initialize $fcn\_map$ which maps $type(node)$ to its corresponding propagation function.
    \State Set $node\_inputs[hidden\_states.grad\_fn] = [R\_out]$

    \While{$stack$}
      \State $curnode \gets stack.pop()$
      \State $curnode\_in\_rel = node\_inputs[curnode]$
      \If{$curnode == target\_node$}
        \Return $R_{in} := curnode\_in\_rel$
      \EndIf
      \State $prop\_fcn \gets fcn\_map[type(curnode)]$
      \State $curnode\_outputs \gets prop\_fcn(curnode\_in\_rel)$
      \For{each $child, output \in zip(out\_adj[curnode], curnode\_outputs)$}
        \State $node\_inputs[child].add(output)$ \footnotemark
        \State $nodes\_pending[child] \gets nodes\_pending[child] - 1$
        \If{$nodes\_pending[child] == 0$}
          \State $stack.push(child)$
        \EndIf
      \EndFor
    \EndWhile
  \end{algorithmic}
\end{algorithm}
\footnotetext{Implementation details omitted for brevity; assume the system tracks the number of accumulated input relevances and aggregates them when all are available.}

\subsection{A Caveat to Autograd}

While the above solution seems sound, a subtle issue lies within defining the propagation functions for each Node type.

Consider a simple approach for backpropagating $R_{c}$ to $R_{a}$ and $R_{b}$ for $c = a + b$ as follows:

\begin{equation}
  R_{a} = R_{c} \times \frac{a^2}{a^2 + b^2} \quad\text{and}\quad R_{b} = R_{c} \times \frac{b^2}{a^2 + b^2}
\end{equation}

or alternatively,

\begin{equation}
  R_{a} = R_{c} \times \frac{a.abs()}{a.abs() + b.abs()} \quad\text{and}\quad R_{b} = R_{c} \times \frac{b.abs()}{a.abs() + b.abs()}
\end{equation}

We suggest this because we want to attribute relevance to each tensor component proportionally to the magnitude of its contribution to the operation result.

Since autograd is a differentiation library, it only caches within its Nodes information necessary for computing the gradients w.r.t. its arguments.

But for $c = a + b$, we see that $\frac{\partial a}{\partial c} = \frac{\partial b}{\partial c} = 1$. This implies that $a$ and $b$ need not be stored in any autograd `AddBackward0' Node, which indeed is the case.

This poses a critical problem for us, since without $a$ and $b$, we are then unable to compute $R_{a}$ and $R_{b}$ faithfully when we traverse the `AddBackward0` Node.

\subsection{The Promise System}

When we reach a Node in traversal where propagation would halt from such a case, we instantiate a \textbf{Promise},
which defers the propagation computations and retrieves the missing tensors from further down in the graph.

Conceptually, a Promise acts as a \textit{placeholder} for missing activations. When an operation requires an unavailable
tensor to compute relevance, the Promise suspends its propagation but continues propagating through the graph to recover
the needed values. Once these are found, the relevance propagation at the Node in question is computed, and the
propagation "catches up" across all the Nodes that were traversed during the search, without backtracking and retraversal
of the computation graph.
We will now formally define this system.\\

\begin{definition}[Promise]
  A Promise is a mutable metadata object that is attached to a Node which requires some uncached forward pass input to compute its relevance propagation.
  The core structure of a Promise object is defined below:
  \begin{verbatim}
    {
      "rout": R_out_curnode,
      "args": [ None ] * num_missing_inputs,
      "rins": [ None ] * num_missing_inputs
    }
  \end{verbatim}
\end{definition}

\begin{definition}[Promise Origin Node]
  A Promise's Origin Node refers to the Node for which the Promise was created.\\
\end{definition}

\begin{definition}[Promise Branch]
  A Promise branch is always tied to a Promise, corresponding to exactly 1 missing forward input in the Origin Node.
  Sibling Promise Branches all share full access to their corresponding Promise object.\\
\end{definition}

The objective of a Promise is to act as a placeholder while we continue traversing the graph in search of the Origin Node's missing values.
\\
\begin{definition}[Promise Branch Arg Node]
  A Promise Branch's Arg Node refers to the first Node along a Promise Branch in its forward pass output is retrievable from its own cached tensors.\\
\end{definition}

Each intermittent Node between the Origin and Arg Nodes contributes two closures, one for its forward
operation and one for its relevance rule. These form a pair of executable chains, one forward, one backward.
When an Arg Node is reached, its forward output tensor is extracted and iteratively passed through the forward
function chain to reconstruct the activation at the Origin Node, which is then stored within the Promise. Once
all Arg Nodes of a Promise have been resolved, the Origin Node’s deferred relevance can be “fast-forwarded” down
each Branch via its backward chain in the same manner. This mechanism effectively defers propagation until all
dependencies are satisfied, ensuring continuity without backtracking or retraversal of the computation graph.

This leads to an amended version of the previous propagation algorithm, Algorithm~\ref{alg:promise-propagation}
(Note that this algorithm is simplified from the algorithm used in practice for the sake of understanding).

\begin{algorithm}[H]
  \caption{Operation-Level Relevance Propagation With Promises}
  \label{alg:promise-propagation}
  \begin{algorithmic}[1]
    \Require Model output $hidden\_states$, Relevance interpretation target $target\_node$, Auxiliary graph $in\_adj$, $out\_adj$, Output relevance $R_{out}$
    \Ensure Input relevance $R_{in}$
    \State Initialize as in Algorithm 2

    \While{$stack$}
      \State $curnode \gets stack.pop()$
      \State $curnode\_in\_rel = node\_inputs[curnode]$
      \If{$curnode == target\_node$}
        \Return $R_{in} := curnode\_in\_rel$
      \ElsIf{$curnode$ requires Promise}
        \State $curnode.promise \gets create\_new\_promise(type(curnode), R_{out})$
        \State $curnode\_outputs \gets curnode.promise.branches$
      \Else
        \State $prop\_fcn \gets fcn\_map[type(curnode)]$
        \State $curnode\_outputs \gets prop\_fcn(curnode\_in\_rel)$
      \EndIf
      \For{each $child, output \in zip(out\_adj[curnode], curnode\_outputs)$}
        \State $node\_inputs[child].add(output)$
        \State $nodes\_pending[child] \gets nodes\_pending[child] - 1$
        \If{$nodes\_pending[child] == 0$}
          \State $stack.push(child)$
        \EndIf
      \EndFor
    \EndWhile
  \end{algorithmic}
\end{algorithm}

We extend the standard propagation functions to handle Promise inputs. Depending on whether the current node is an
Arg Node, the function either records the chain or resolves and completes the Promise.

\begin{algorithm}[H]
  \caption{Non-Arg Node Propagation Function Promise Handling}
  \label{alg:non-argnode-propagation}
  \begin{algorithmic}[1]
    \Require Autograd Node $node$, Propagation input $R_{out}$
    \Ensure Propagation output list $R_{in}$

    \If{$R_{out}$ is a Promise Branch}
      \State Define $fwd$ as a closure of $node$'s forward pass.
      \State Define $bwd$ as a closure of $node$'s relevance distribution logic.
      \State $R_{out}.record(fwd, bwd)$
      \Return $R_{out}$
    \EndIf

    \State \textit{// Propagate $R_{out}$...}
    \Return $R_{in}$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Arg Node Propagation Function Promise Handling}
  \label{alg:argnode-propagation}
  \begin{algorithmic}[1]
    \Require Autograd Node $node$, Propagation input $R_{out}$
    \Ensure Propagation output list $R_{in}$

    \If{$R_{out}$ is a Promise Branch}
      \State Define $retrieve_fwd_output$ as a function that extracts $node$'s forward pass output given its saved tensors.
      \State $activation = retrieve_fwd_output(node)$
      \State $R_{out}.set_arg(activation)$
      \State $R_{out}.trigger_promise_completion()$
      \If{$R_{out}.promise_is_complete$}
        $R_{out} = R_{out}.propagated_relevance$
      \Else
        \State \textit{// Signal for queueing this Node until the Promise is complete}
        \Return
      \EndIf
    \EndIf

    \State \textit{// Propagate $R_{out}$...}
    \Return $R_{in}$
  \end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}

We analyze the computational complexity of our promise-based LRP algorithm in terms of the architectural properties that determine promise resolution requirements.

\begin{definition}[Promise-Generating Operations]
Let $P \subseteq V$ be the set of promise-generating operations in the computation graph $G = (V, E)$, defined as:
$$P = \{v \in V : \text{type}(v) \in \{\text{AddBackward}, \text{SumBackward}, \text{CatBackward}, \text{UnbindBackward}, \text{MeanBackward}, \text{StackBackward}\}\}$$
These operations may lack required forward activations during LRP traversal, necessitating promise-based resolution.
\end{definition}

\begin{definition}[Promise Density]
The promise density of a neural network architecture is defined as:
$$\rho = \frac{|P|}{|V|}$$
where $|P|$ is the number of promise-generating operations and $|V|$ is the total number of operations in the computation graph.
\end{definition}

\begin{definition}[Promise Depth]
For each promise-generating operation $p \in P$, let $d(p)$ be the graph distance from $p$ to the nearest operation that stores the forward activation required by $p$. The maximum promise depth is:
$$D = \max_{p \in P} d(p)$$
\end{definition}

\begin{theorem}[Promise-Based LRP Complexity]
Let $G = (V, E)$ be the computation graph of a neural network with $n = |V|$ operations, $m = |E|$ edges, promise-generating set $P$, and maximum promise depth $D$. The promise-based LRP algorithm requires $F = 1 + |P| \cdot D$ forward passes and $B = 1$ backward pass, giving total complexity $O((1 + |P| \cdot D) \cdot n + m)$.
\end{theorem}

\begin{proof}
We analyze the algorithm in three distinct phases:

\textbf{Phase 1 - Initial Forward Pass:} The algorithm performs one forward pass to construct the computation graph $G$ and identify all promise-generating operations $P$. This requires $O(n + m)$ time.

\textbf{Phase 2 - Promise Resolution:} When the LRP traversal encounters a promise-generating operation $p \in P$ that lacks its required forward activation, a promise object is created. Each promise must traverse backward through the graph until it locates the operation storing its required activation. 

Crucially, our implementation shows that promises cannot share activations even when they require the same values. Each promise maintains independent traversal paths (via distinct \texttt{fwd\_list}, \texttt{bwd\_list}, and \texttt{arg\_node\_ind} fields) and resolves separately through individual \texttt{setarg()} calls. Therefore, each of the $|P|$ promises may require up to $D$ traversal steps, leading to $|P| \cdot D$ additional forward computations in the worst case.

\textbf{Phase 3 - Backward LRP Pass:} Once all promises are resolved and forward activations are reconstructed, the algorithm performs exactly one backward pass through $G$ in reverse topological order to compute relevance propagation. This requires $O(n + m)$ time.

\textbf{Architectural Dependence:} The parameters $|P|$ and $D$ are fundamentally determined by the network architecture:
\begin{itemize}
\item \textbf{Feed-forward architectures} (e.g., basic CNNs): $|P| \approx 0 \Rightarrow \rho \approx 0$, giving optimal $O(n + m)$ complexity.
\item \textbf{Residual architectures} (e.g., ResNets): $|P|$ proportional to the number of skip connections, $D$ bounded by the maximum skip distance.
\item \textbf{Transformer architectures}: $|P|$ grows with the number of attention layers and residual connections, $D$ typically bounded by layer depth.
\end{itemize}

Therefore, the total complexity is $O((1 + |P| \cdot D) \cdot n + m)$, where both $|P|$ and $D$ depend on architectural choices but are independent of input size.
\end{proof}

\section{Experimental Setup}
We aim to show through the following experiments that our proposed LRP solution remains mathematically sound and produces
faithful and interpretable attributions consistent with existing methods. We also demonstrate that the
Promise-based LRP maintains a predictable and efficient memory footprint and sufficient computation speed for
research applications on large (100s of millions - 1 billion parameters) models of various architectures
using consumer-grade hardware.

\subsection{Model and Dataset Selection}
We used RoBERTa-large finetuned on the SQuAD-v2 question-answering dataset, DNABERT-2 finetuned on the GUE
epigenetic marker prediction (EMP) task and H3 target, Llama-3.2-1B finetuned on the IMDB movie review dataset,
and the ViT-b-16 and VGG16 base models to evaluate on the CIFAR-10 ImageNet dataset. Finetuned models were then used to
evaluate examples from their respective datasets' validation splits.

\subsection{Visualizing Attributions for Image Models}
\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{../documentation/VGG_attributions.png}
  \caption{A comparison across various attribution methods and our LRP.}
\end{figure}
\bibliography{main}
\iftoggle{arxiv}{
  % arXiv references in a generic style, or ICLR style
  \bibliographystyle{iclr2025_conference}
}{
  % bst style for the journal/conference version goes here
  % For NeurIPS there is no house bst file; you can use ICLR bst instead
  % \bibliographystyle{iclr2025_conference}
}

\appendix
% Appendix TOC
\section*{Appendices}
%\startcontents[sections]
%\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
\subsection{Implementation Details}
Our implementation uses PyTorch hooks to record operations during the forward pass and constructs the computation graph dynamically. Each operation creates a promise object, which stores the propagation rule and tracks readiness and completeness.

\textbf{Python Example: Promise-based LRP Engine}
Below is a simplified Python pseudocode that captures the core logic of the promise-based LRP engine:

\begin{verbatim}
class Promise:
  def __init__(self, op_type, inputs):
    self.op_type = op_type
    self.inputs = inputs
    self.ready = False
    self.complete = False
    self.relevance = None

  def resolve(self, output_relevance):
    # Custom propagation rule for each op_type
    self.relevance = propagate_relevance(self.op_type, self.inputs, output_relevance)
    self.ready = True
    return self.relevance

def lrp_engine(graph, output_relevance):
  # Traverse graph in reverse topological order
  for node in reversed(graph.topo_order()):
    promise = node.promise
    if all(child.promise.complete for child in node.outputs):
      input_relevance = promise.resolve(node.output_relevance)
      for inp in node.inputs:
        inp.promise.relevance = input_relevance[inp]
      promise.complete = True
  return [node.promise.relevance for node in graph.input_nodes()]
\end{verbatim}

\textbf{Explicit Steps in the Algorithm}
\begin{enumerate}
  \item \textbf{Forward Pass:} Register hooks to record each operation and build the computation graph.
  \item \textbf{Promise Creation:} For each operation, instantiate a promise object with its propagation rule.
  \item \textbf{Backward Pass:} Starting from the output, traverse the graph in reverse topological order. Each promise is resolved when all output relevance is received, and marked complete when it distributes relevance to its inputs.
  \item \textbf{Relevance Aggregation:} Input nodes collect relevance from all incoming promises, yielding the final attribution scores.
\end{enumerate}

\textbf{Readiness and Completeness}
\begin{itemize}
  \item \textbf{Readiness:} A promise is ready when all output relevance (from downstream nodes) has been received.
  \item \textbf{Completeness:} A promise is complete when it has distributed its relevance to all inputs according to its propagation rule.
\end{itemize}

This mechanism ensures that relevance is propagated in accordance with the mathematical definition of LRP, maintaining conservation and proportionality at each node.
\end{document}
