{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c1e774-78ed-4966-a00c-4ea2b12d30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95de822-b524-4216-94cc-01a98d1361c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "run_our_lrp = True\n",
    "attn_eager = False\n",
    "use_bf16 = False\n",
    "if use_bf16:\n",
    "    lrp_dtype = torch.bfloat16\n",
    "    dtype_name = \"bf16\"\n",
    "else:\n",
    "    lrp_dtype = torch.float32\n",
    "    dtype_name = \"f32\"\n",
    "if run_our_lrp:\n",
    "    lrp_name = \"our\"\n",
    "else:\n",
    "    lrp_name = \"attn\"\n",
    "if attn_eager:\n",
    "    attn_mode = \"eager\"\n",
    "else:\n",
    "    attn_mode = \"sdpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959c5669-6748-4a90-a2df-916385342e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not run_our_lrp:\n",
    "    from lxt.explicit.models.llama import LlamaForCausalLM, attnlrp\n",
    "    from lxt.utils import pdf_heatmap, clean_tokens\n",
    "    import transformers\n",
    "else:\n",
    "    from transformers import LlamaForCausalLM\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from lrp_engine import LRPEngine\n",
    "\n",
    "path = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path, torch_dtype=lrp_dtype, device_map=\"cuda\", attn_implementation=attn_mode)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "if not run_our_lrp:\n",
    "    # apply AttnLRP rules\n",
    "    attnlrp.register(model)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d6e00-d269-4364-a344-aee61b7acb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llama(model, tokenizer, input_ids, lrp=None):\n",
    "    # get input embeddings so that we can compute gradients w.r.t. input embeddings\n",
    "    input_embeds = model.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    # inference and get the maximum logit at the last position\n",
    "    output_logits = model(inputs_embeds=input_embeds.requires_grad_()).logits\n",
    "    max_logits, max_indices = torch.max(output_logits[0, -1, :], dim=-1)\n",
    "    \n",
    "    # get the top k tokens and their logits\n",
    "    topk_logits, topk_indices = torch.topk(output_logits[0, -1, :], k=5, dim=-1)\n",
    "    \n",
    "    # convert token indices to strings\n",
    "    topk_tokens = tokenizer.convert_ids_to_tokens(topk_indices)\n",
    "    \n",
    "    # initialize relevance scores with max_logits itself and backpropagate\n",
    "    if lrp is not None:\n",
    "        relevance = lrp.run(max_logits.unsqueeze(0))\n",
    "        relevance = relevance[1][0][0].float().cpu()\n",
    "    else:\n",
    "        max_logits.backward(max_logits)\n",
    "        relevance = input_embeds.grad.float().sum(-1).cpu()[0] # cast to float32 before summation for higher precision\n",
    "    \n",
    "    # normalize relevance between [-1, 1] for plotting\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "\n",
    "    # print(relevance.topk(5))\n",
    "\n",
    "    topk_indices = topk_indices.tolist()\n",
    "    non_start_end_tok_relevance = relevance\n",
    "    non_start_end_tok_relevance[0] = 0.0\n",
    "    non_start_end_tok_relevance[-1] = 0.0\n",
    "    topk_attr_inds = input_ids[0][non_start_end_tok_relevance.topk(5).indices].tolist()\n",
    "    top_attr_ind = topk_attr_inds[0]\n",
    "    union = len(set(topk_attr_inds).union(set(topk_indices)))\n",
    "    intersect = len(set(topk_attr_inds).intersection(set(topk_indices)))\n",
    "\n",
    "    # print(topk_attr_inds, topk_indices)\n",
    "    # print(tokenizer.convert_ids_to_tokens(topk_attr_inds), tokenizer.convert_ids_to_tokens(topk_indices))\n",
    "\n",
    "    top1_hit = top_attr_ind == topk_indices[0]\n",
    "\n",
    "    return top1_hit, relevance#, union, intersect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcbedba-8a05-40fe-92a9-68c1be21ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1580e9d1-3804-409e-ae09-e1d30f07f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_our_lrp:\n",
    "    lrp = LRPEngine(dtype=lrp_dtype)\n",
    "else:\n",
    "    lrp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca5c21a-3ef3-426e-b93c-087deacd8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [ example for example in dataset[\"validation\"] if example[\"answers\"][\"text\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a548e6d-f569-47f5-b7d2-eac6ed30ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[0]\n",
    "question = example[\"question\"]\n",
    "context = example[\"context\"]\n",
    "prompt = context + \" \" + question\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ab2731-5c42-4af1-967c-4d3a8fe3937a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Warmup iteration\n",
    "_, _ = evaluate_llama(model, tokenizer, input_ids, lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "374bed5a-a583-44ac-8d46-58edc5a27959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lrp.promise_bucket.start_nodes_to_promise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a3101b-53e4-4dc5-b0bb-169aa2a19d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "top1_label_hits = 0\n",
    "top1_model_hits = 0\n",
    "total_examples = 0\n",
    "total_intersect = 0\n",
    "total_union = 0\n",
    "\n",
    "for example in tqdm(ds):\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = example[\"answers\"][\"text\"]\n",
    "\n",
    "    prompt = context + \" \" + question\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)\n",
    "    if input_ids.shape[-1] > 512:\n",
    "        continue\n",
    "    \n",
    "    top1_hit, relevance = evaluate_llama(model, tokenizer, input_ids, lrp)\n",
    "    # print(answers)\n",
    "    if top1_hit:\n",
    "        top1_model_hits += 1\n",
    "    total_examples += 1\n",
    "\n",
    "    results.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attr\": relevance.detach().cpu(),\n",
    "    })\n",
    "    \n",
    "    if not (total_examples % 100):\n",
    "        print(top1_model_hits, total_examples, top1_model_hits / total_examples)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f61eb19-48c1-4386-9e95-4bfded7c3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxt.utils import pdf_heatmap, clean_tokens\n",
    "os.chdir(\"heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2766fc-cda6-4d7c-bf8b-04f00865bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results):\n",
    "    relevance = result[\"attr\"]\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(result[\"input_ids\"][0])\n",
    "    tokens = clean_tokens(tokens)\n",
    "    pdf_heatmap(tokens, relevance, path=f'{lrp_name}lrp_{dtype_name}_{attn_mode}_heatmap_{i}.pdf', backend='xelatex')\n",
    "    # pdf_heatmap(tokens, relevance, path=f'{lrp_name}lrp_single_{attn_mode}_test2.pdf', backend='xelatex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dd308-51fa-4c4b-bc3f-feb38369fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top1_model_hits / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c714d1-578e-48e6-8265-abfb1b016b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
