{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a883663-bcdd-4b5a-aad6-8b1e259ca233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafe2229-62f4-47ec-a47b-138cab862eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe7781a-9096-41d0-9fc0-7af08a49a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\7bce263b15377fc15361f52cfab88f8b586abda0\\bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a235408-c5d2-406b-8bc8-020780f435e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_file_path = \"../GUE_finetuned_models/DNABERT2_EMP_H3_model/model.safetensors\"\n",
    "loaded_state_dict = load_file(model_file_path, device=device)\n",
    "model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b242dae5-d863-4b88-b0c0-9bbaf700d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrp import checkpoint_hook, lrp_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88660e0-3eb4-41fa-bcef-16f80737abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_module in model.bert.encoder.layer:\n",
    "    layer_module.attention.self.register_forward_hook(checkpoint_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeea66ba-0e48-4848-bc27-78acb3fdfffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../GUE/EMP/H3/test.csv\"\n",
    "batch_size = 500\n",
    "batch_start = 0\n",
    "with open(data_path, \"r\") as fileIn:\n",
    "    fileIn.readline()\n",
    "    dna = []\n",
    "    for _ in range(batch_start):\n",
    "        fileIn.readline()\n",
    "    for _ in range(batch_size):\n",
    "        line = fileIn.readline()\n",
    "        line = line.split(\",\")[0].strip()\n",
    "        dna.append(line)\n",
    "\n",
    "inputs = sorted([ tokenizer(x, return_tensors = 'pt')[\"input_ids\"] for x in dna ], key=lambda t: t.shape[-1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3509754-29b3-42b8-a5c0-0f696789623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd71d44-6416-4a87-b17d-9a57961d9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_adj_list = None\n",
    "out_adj_list = None\n",
    "topo_exec_order = None\n",
    "fcn_map = None\n",
    "agg_checkpoint_vals = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b43375f-b546-41e1-8bc2-ce7107bea1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propagation took 1.0231122970581055 seconds\n",
      "tensor(3.8319, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Single example test\n",
    "outputs : torch.Tensor = model(inputs[0].to(device))\n",
    "logits = outputs[0]\n",
    "hidden_states = outputs.hidden_states\n",
    "checkpoint_vals, out_adj_list, topo_exec_order, fcn_map = lrp_engine(hidden_states.half(), out_adj_list, topo_exec_order, fcn_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a039ebdc-e4d4-4344-91e9-7aaee4550ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-generated, needed quick tooling\n",
    "# Basically adding two matrices together if dim 0 is variable, but dim 1 is constant.\n",
    "def accumulate_variable_matrix(accumulator, new_matrix):\n",
    "    # Get current shapes\n",
    "    acc_rows, acc_cols = accumulator.shape\n",
    "    new_rows, new_cols = new_matrix.shape\n",
    "\n",
    "    # Determine target size (assume row counts always match, or adapt as needed)\n",
    "    target_rows = max(acc_rows, new_rows)\n",
    "\n",
    "    # Expand accumulator if needed\n",
    "    if acc_rows < target_rows:\n",
    "        pad_rows = target_rows - acc_rows\n",
    "        accumulator = torch.cat([accumulator, torch.zeros(pad_rows, acc_cols, device=accumulator.device, dtype=accumulator.dtype)], dim=0)\n",
    "\n",
    "    # Expand new_matrix if needed\n",
    "    if new_rows < target_rows:\n",
    "        pad_rows = target_rows - new_rows\n",
    "        new_matrix = torch.cat([new_matrix, torch.zeros(pad_rows, new_cols, device=accumulator.device, dtype=accumulator.dtype)], dim=0)\n",
    "\n",
    "    # Now both matrices have same shape, so they can be added\n",
    "    accumulator += new_matrix\n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f33b2662-5a87-414e-a7c9-2e39590fb04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propagation took 0.6579298973083496 seconds\n",
      "propagation took 0.5240142345428467 seconds\n",
      "propagation took 0.4928596019744873 seconds\n",
      "propagation took 0.5079989433288574 seconds\n",
      "propagation took 0.5044794082641602 seconds\n",
      "propagation took 0.4895188808441162 seconds\n",
      "propagation took 0.5313961505889893 seconds\n",
      "propagation took 0.5046494007110596 seconds\n",
      "propagation took 0.5338771343231201 seconds\n",
      "propagation took 0.5186073780059814 seconds\n",
      "propagation took 0.5047578811645508 seconds\n",
      "propagation took 0.5042538642883301 seconds\n",
      "propagation took 0.524308443069458 seconds\n",
      "propagation took 0.5159878730773926 seconds\n",
      "propagation took 0.5194118022918701 seconds\n",
      "propagation took 0.5113911628723145 seconds\n",
      "propagation took 0.5190114974975586 seconds\n",
      "propagation took 0.5017216205596924 seconds\n",
      "propagation took 0.4765148162841797 seconds\n",
      "propagation took 0.46941590309143066 seconds\n",
      "propagation took 0.526705265045166 seconds\n",
      "propagation took 0.5289864540100098 seconds\n",
      "propagation took 0.520104169845581 seconds\n",
      "propagation took 0.520585298538208 seconds\n",
      "propagation took 0.5307648181915283 seconds\n",
      "propagation took 0.5056607723236084 seconds\n",
      "propagation took 0.4880239963531494 seconds\n",
      "propagation took 0.5286009311676025 seconds\n",
      "propagation took 0.547020673751831 seconds\n",
      "propagation took 0.5399668216705322 seconds\n",
      "propagation took 0.5083913803100586 seconds\n",
      "propagation took 0.5450069904327393 seconds\n",
      "propagation took 0.5855312347412109 seconds\n",
      "propagation took 0.6070051193237305 seconds\n",
      "propagation took 0.5133278369903564 seconds\n",
      "propagation took 0.5151965618133545 seconds\n",
      "propagation took 0.5050144195556641 seconds\n",
      "propagation took 0.645010232925415 seconds\n",
      "propagation took 0.517988920211792 seconds\n",
      "propagation took 0.5076065063476562 seconds\n",
      "propagation took 0.49599361419677734 seconds\n",
      "propagation took 0.4979848861694336 seconds\n",
      "propagation took 0.588897705078125 seconds\n",
      "propagation took 0.5276000499725342 seconds\n",
      "propagation took 0.506234884262085 seconds\n",
      "propagation took 0.511939525604248 seconds\n",
      "propagation took 0.50498366355896 seconds\n",
      "propagation took 0.4799985885620117 seconds\n",
      "propagation took 0.5247650146484375 seconds\n",
      "propagation took 0.5327348709106445 seconds\n",
      "propagation took 0.4888942241668701 seconds\n",
      "propagation took 0.4916532039642334 seconds\n",
      "propagation took 0.4820539951324463 seconds\n",
      "propagation took 0.5319056510925293 seconds\n",
      "propagation took 0.5053839683532715 seconds\n",
      "propagation took 0.5259878635406494 seconds\n",
      "propagation took 0.5352292060852051 seconds\n",
      "propagation took 0.49214911460876465 seconds\n",
      "propagation took 0.5399539470672607 seconds\n",
      "propagation took 0.5031335353851318 seconds\n",
      "propagation took 0.5108411312103271 seconds\n",
      "propagation took 0.4957404136657715 seconds\n",
      "propagation took 0.5648078918457031 seconds\n",
      "propagation took 0.4879939556121826 seconds\n",
      "propagation took 0.5354483127593994 seconds\n",
      "propagation took 0.5600581169128418 seconds\n",
      "propagation took 0.5110635757446289 seconds\n",
      "propagation took 0.5569593906402588 seconds\n",
      "propagation took 0.5080230236053467 seconds\n",
      "propagation took 0.49524664878845215 seconds\n",
      "propagation took 0.5288400650024414 seconds\n",
      "propagation took 0.4986090660095215 seconds\n",
      "propagation took 0.49370908737182617 seconds\n",
      "propagation took 0.5335781574249268 seconds\n",
      "propagation took 0.594304084777832 seconds\n",
      "propagation took 0.5312347412109375 seconds\n",
      "propagation took 0.513714075088501 seconds\n",
      "propagation took 0.5018622875213623 seconds\n",
      "propagation took 0.5109117031097412 seconds\n",
      "propagation took 0.5151903629302979 seconds\n",
      "propagation took 0.514134407043457 seconds\n",
      "propagation took 0.537489652633667 seconds\n",
      "propagation took 0.5167980194091797 seconds\n",
      "propagation took 0.4959993362426758 seconds\n",
      "propagation took 0.5382583141326904 seconds\n",
      "propagation took 0.578193187713623 seconds\n",
      "propagation took 0.5227806568145752 seconds\n",
      "propagation took 0.5310258865356445 seconds\n",
      "propagation took 0.5198283195495605 seconds\n",
      "propagation took 0.5730009078979492 seconds\n",
      "propagation took 0.5595452785491943 seconds\n",
      "propagation took 0.637000560760498 seconds\n",
      "propagation took 0.5154645442962646 seconds\n",
      "propagation took 0.5704178810119629 seconds\n",
      "propagation took 0.4955775737762451 seconds\n",
      "propagation took 0.5113427639007568 seconds\n",
      "propagation took 0.5416774749755859 seconds\n",
      "propagation took 0.5028457641601562 seconds\n",
      "propagation took 0.4979102611541748 seconds\n",
      "propagation took 0.530968427658081 seconds\n",
      "propagation took 0.5197856426239014 seconds\n",
      "propagation took 0.5219113826751709 seconds\n",
      "propagation took 0.5059723854064941 seconds\n",
      "propagation took 0.5173077583312988 seconds\n",
      "propagation took 0.53299880027771 seconds\n",
      "propagation took 0.5175073146820068 seconds\n",
      "propagation took 0.5119998455047607 seconds\n",
      "propagation took 0.5059993267059326 seconds\n",
      "propagation took 0.514000654220581 seconds\n",
      "propagation took 0.5050003528594971 seconds\n",
      "propagation took 0.48000049591064453 seconds\n",
      "propagation took 0.5340001583099365 seconds\n",
      "propagation took 0.5250003337860107 seconds\n",
      "propagation took 0.5090000629425049 seconds\n",
      "propagation took 0.4960002899169922 seconds\n",
      "propagation took 0.5199995040893555 seconds\n",
      "propagation took 0.502000093460083 seconds\n",
      "propagation took 0.5113835334777832 seconds\n",
      "propagation took 0.5102279186248779 seconds\n",
      "propagation took 0.5160002708435059 seconds\n",
      "propagation took 0.5070004463195801 seconds\n",
      "propagation took 0.4830055236816406 seconds\n",
      "propagation took 0.520010232925415 seconds\n",
      "propagation took 0.500999927520752 seconds\n",
      "propagation took 0.507474422454834 seconds\n",
      "propagation took 0.548825740814209 seconds\n",
      "propagation took 0.5229506492614746 seconds\n",
      "propagation took 0.5079505443572998 seconds\n",
      "propagation took 0.49780845642089844 seconds\n",
      "propagation took 0.5146408081054688 seconds\n",
      "propagation took 0.509108304977417 seconds\n",
      "propagation took 0.5071337223052979 seconds\n",
      "propagation took 0.5318677425384521 seconds\n",
      "propagation took 0.5198013782501221 seconds\n",
      "propagation took 0.5060214996337891 seconds\n",
      "propagation took 0.48135995864868164 seconds\n",
      "propagation took 0.511265754699707 seconds\n",
      "propagation took 0.5112166404724121 seconds\n",
      "propagation took 0.5202100276947021 seconds\n",
      "propagation took 0.5389299392700195 seconds\n",
      "propagation took 0.5269985198974609 seconds\n",
      "propagation took 0.49773716926574707 seconds\n",
      "propagation took 0.5049018859863281 seconds\n",
      "propagation took 0.5161395072937012 seconds\n",
      "propagation took 0.527031421661377 seconds\n",
      "propagation took 0.5251924991607666 seconds\n",
      "propagation took 0.5178115367889404 seconds\n",
      "propagation took 0.5117216110229492 seconds\n",
      "propagation took 0.5165379047393799 seconds\n",
      "propagation took 0.5048081874847412 seconds\n",
      "propagation took 0.5732409954071045 seconds\n",
      "propagation took 0.6367859840393066 seconds\n",
      "propagation took 0.5452196598052979 seconds\n",
      "propagation took 0.5002546310424805 seconds\n",
      "propagation took 0.5081853866577148 seconds\n",
      "propagation took 0.4859628677368164 seconds\n",
      "propagation took 0.5424511432647705 seconds\n",
      "propagation took 0.4974250793457031 seconds\n",
      "propagation took 0.5028326511383057 seconds\n",
      "propagation took 0.5037679672241211 seconds\n",
      "propagation took 0.537076473236084 seconds\n",
      "propagation took 0.5069944858551025 seconds\n",
      "propagation took 0.5260286331176758 seconds\n",
      "propagation took 0.49924778938293457 seconds\n",
      "propagation took 0.5000154972076416 seconds\n",
      "propagation took 0.5489010810852051 seconds\n",
      "propagation took 0.5504510402679443 seconds\n",
      "propagation took 0.5255510807037354 seconds\n",
      "propagation took 0.5489804744720459 seconds\n",
      "propagation took 0.5119621753692627 seconds\n",
      "propagation took 0.5028343200683594 seconds\n",
      "propagation took 0.5209691524505615 seconds\n",
      "propagation took 0.4908487796783447 seconds\n",
      "propagation took 0.5211174488067627 seconds\n",
      "propagation took 0.5312457084655762 seconds\n",
      "propagation took 0.5160923004150391 seconds\n",
      "propagation took 0.516226053237915 seconds\n",
      "propagation took 0.5115699768066406 seconds\n",
      "propagation took 0.5470383167266846 seconds\n",
      "propagation took 0.49932241439819336 seconds\n",
      "propagation took 0.5035514831542969 seconds\n",
      "propagation took 0.5095510482788086 seconds\n",
      "propagation took 0.5057721138000488 seconds\n",
      "propagation took 0.5228650569915771 seconds\n",
      "propagation took 0.5228288173675537 seconds\n",
      "propagation took 0.5299603939056396 seconds\n",
      "propagation took 0.5009989738464355 seconds\n",
      "propagation took 0.508852481842041 seconds\n",
      "propagation took 0.5140471458435059 seconds\n",
      "propagation took 0.5139012336730957 seconds\n",
      "propagation took 0.49433445930480957 seconds\n",
      "propagation took 0.509049654006958 seconds\n",
      "propagation took 0.5416758060455322 seconds\n",
      "propagation took 0.5043089389801025 seconds\n",
      "propagation took 0.5139977931976318 seconds\n",
      "propagation took 0.5248613357543945 seconds\n",
      "propagation took 0.5384199619293213 seconds\n",
      "propagation took 0.531203031539917 seconds\n",
      "propagation took 0.5191574096679688 seconds\n",
      "propagation took 0.49925732612609863 seconds\n",
      "propagation took 0.49496889114379883 seconds\n",
      "propagation took 0.48896193504333496 seconds\n",
      "propagation took 0.5267946720123291 seconds\n",
      "propagation took 0.513014554977417 seconds\n",
      "propagation took 0.4979209899902344 seconds\n",
      "propagation took 0.5819544792175293 seconds\n",
      "propagation took 0.5080785751342773 seconds\n",
      "propagation took 0.49834513664245605 seconds\n",
      "propagation took 0.5929336547851562 seconds\n",
      "propagation took 0.526461124420166 seconds\n",
      "propagation took 0.5114150047302246 seconds\n",
      "propagation took 0.5174446105957031 seconds\n",
      "propagation took 0.5533649921417236 seconds\n",
      "propagation took 0.5881698131561279 seconds\n",
      "propagation took 0.5865120887756348 seconds\n",
      "propagation took 0.4884812831878662 seconds\n",
      "propagation took 0.548922061920166 seconds\n",
      "propagation took 0.5191071033477783 seconds\n",
      "propagation took 0.5189435482025146 seconds\n",
      "propagation took 0.6511590480804443 seconds\n",
      "propagation took 0.5260224342346191 seconds\n",
      "propagation took 0.5331547260284424 seconds\n",
      "propagation took 0.5793163776397705 seconds\n",
      "propagation took 0.5573666095733643 seconds\n",
      "propagation took 0.5565292835235596 seconds\n",
      "propagation took 0.509727954864502 seconds\n",
      "propagation took 0.5049197673797607 seconds\n",
      "propagation took 0.5236108303070068 seconds\n",
      "propagation took 0.5174214839935303 seconds\n",
      "propagation took 0.5240495204925537 seconds\n",
      "propagation took 0.5138256549835205 seconds\n",
      "propagation took 0.5084269046783447 seconds\n",
      "propagation took 0.5342144966125488 seconds\n",
      "propagation took 0.5549893379211426 seconds\n",
      "propagation took 0.525477409362793 seconds\n",
      "propagation took 0.5099997520446777 seconds\n",
      "propagation took 0.564000129699707 seconds\n",
      "propagation took 0.5598962306976318 seconds\n",
      "propagation took 0.5059030055999756 seconds\n",
      "propagation took 0.5364220142364502 seconds\n",
      "propagation took 0.5288941860198975 seconds\n",
      "propagation took 0.5278964042663574 seconds\n",
      "propagation took 0.5284998416900635 seconds\n",
      "propagation took 0.4947187900543213 seconds\n",
      "propagation took 0.5249664783477783 seconds\n",
      "propagation took 0.5044488906860352 seconds\n",
      "propagation took 0.5068836212158203 seconds\n",
      "propagation took 0.5247538089752197 seconds\n",
      "propagation took 0.5173594951629639 seconds\n",
      "propagation took 0.5194392204284668 seconds\n",
      "propagation took 0.48859333992004395 seconds\n",
      "propagation took 0.5491194725036621 seconds\n",
      "propagation took 0.5185515880584717 seconds\n",
      "propagation took 0.5152645111083984 seconds\n",
      "propagation took 0.5123860836029053 seconds\n",
      "propagation took 0.5559933185577393 seconds\n",
      "propagation took 0.5470459461212158 seconds\n",
      "propagation took 0.5292038917541504 seconds\n",
      "propagation took 0.48601603507995605 seconds\n",
      "propagation took 0.5231585502624512 seconds\n",
      "propagation took 0.5168807506561279 seconds\n",
      "propagation took 0.5275301933288574 seconds\n",
      "propagation took 0.6203219890594482 seconds\n",
      "propagation took 0.5725932121276855 seconds\n",
      "propagation took 0.523123025894165 seconds\n",
      "propagation took 0.49900221824645996 seconds\n",
      "propagation took 0.5030455589294434 seconds\n",
      "propagation took 0.47252607345581055 seconds\n",
      "propagation took 0.512871265411377 seconds\n",
      "propagation took 0.5148255825042725 seconds\n",
      "propagation took 0.5290513038635254 seconds\n",
      "propagation took 0.49211955070495605 seconds\n",
      "propagation took 0.5089206695556641 seconds\n",
      "propagation took 0.5010082721710205 seconds\n",
      "propagation took 0.5522520542144775 seconds\n",
      "propagation took 0.5243287086486816 seconds\n",
      "propagation took 0.4929983615875244 seconds\n",
      "propagation took 0.5380027294158936 seconds\n",
      "propagation took 0.5497546195983887 seconds\n",
      "propagation took 0.5312507152557373 seconds\n",
      "propagation took 0.5871288776397705 seconds\n",
      "propagation took 0.525921106338501 seconds\n",
      "propagation took 0.5186409950256348 seconds\n",
      "propagation took 0.5062093734741211 seconds\n",
      "propagation took 0.5320031642913818 seconds\n",
      "propagation took 0.548006534576416 seconds\n",
      "propagation took 0.5221209526062012 seconds\n",
      "propagation took 0.5489981174468994 seconds\n",
      "propagation took 0.5754115581512451 seconds\n",
      "propagation took 0.5376842021942139 seconds\n",
      "propagation took 0.5369222164154053 seconds\n",
      "propagation took 0.5428712368011475 seconds\n",
      "propagation took 0.5127766132354736 seconds\n",
      "propagation took 0.5459036827087402 seconds\n",
      "propagation took 0.5242276191711426 seconds\n",
      "propagation took 0.5213601589202881 seconds\n",
      "propagation took 0.5250017642974854 seconds\n",
      "propagation took 0.5051126480102539 seconds\n",
      "propagation took 0.531639575958252 seconds\n",
      "propagation took 0.5657093524932861 seconds\n",
      "propagation took 0.5601327419281006 seconds\n",
      "propagation took 0.5573694705963135 seconds\n",
      "propagation took 0.5231499671936035 seconds\n",
      "propagation took 0.5063095092773438 seconds\n",
      "propagation took 0.5446908473968506 seconds\n",
      "propagation took 0.5412919521331787 seconds\n",
      "propagation took 0.5239386558532715 seconds\n",
      "propagation took 0.5139739513397217 seconds\n",
      "propagation took 0.5094749927520752 seconds\n",
      "propagation took 0.5134773254394531 seconds\n",
      "propagation took 0.5256369113922119 seconds\n",
      "propagation took 0.5328524112701416 seconds\n",
      "propagation took 0.5240609645843506 seconds\n",
      "propagation took 0.53000807762146 seconds\n",
      "propagation took 0.55303955078125 seconds\n",
      "propagation took 0.55397629737854 seconds\n",
      "propagation took 0.6279878616333008 seconds\n",
      "propagation took 0.5310883522033691 seconds\n",
      "propagation took 0.5359692573547363 seconds\n",
      "propagation took 0.5190081596374512 seconds\n",
      "propagation took 0.5081241130828857 seconds\n",
      "propagation took 0.517683744430542 seconds\n",
      "propagation took 0.5204880237579346 seconds\n",
      "propagation took 0.501960039138794 seconds\n",
      "propagation took 0.5517137050628662 seconds\n",
      "propagation took 0.5044851303100586 seconds\n",
      "propagation took 0.5131142139434814 seconds\n",
      "propagation took 0.5146441459655762 seconds\n",
      "propagation took 0.5190639495849609 seconds\n",
      "propagation took 0.515535831451416 seconds\n",
      "propagation took 0.5233283042907715 seconds\n",
      "propagation took 0.5391104221343994 seconds\n",
      "propagation took 0.5106663703918457 seconds\n",
      "propagation took 0.4921543598175049 seconds\n",
      "propagation took 0.5359151363372803 seconds\n",
      "propagation took 0.5025618076324463 seconds\n",
      "propagation took 0.5301117897033691 seconds\n",
      "propagation took 0.5114498138427734 seconds\n",
      "propagation took 0.5251686573028564 seconds\n",
      "propagation took 0.5078775882720947 seconds\n",
      "propagation took 0.4968423843383789 seconds\n",
      "propagation took 0.523740291595459 seconds\n",
      "propagation took 0.5305724143981934 seconds\n",
      "propagation took 0.5542943477630615 seconds\n",
      "propagation took 0.5350918769836426 seconds\n",
      "propagation took 0.5040569305419922 seconds\n",
      "propagation took 0.5167367458343506 seconds\n",
      "propagation took 0.5074408054351807 seconds\n",
      "propagation took 0.48807501792907715 seconds\n",
      "propagation took 0.505021333694458 seconds\n",
      "propagation took 0.5252594947814941 seconds\n",
      "propagation took 0.5133075714111328 seconds\n",
      "propagation took 0.5277481079101562 seconds\n",
      "propagation took 0.5302162170410156 seconds\n",
      "propagation took 0.5069983005523682 seconds\n",
      "propagation took 0.5236618518829346 seconds\n",
      "propagation took 0.5196459293365479 seconds\n",
      "propagation took 0.5137419700622559 seconds\n",
      "propagation took 0.4960169792175293 seconds\n",
      "propagation took 0.4992814064025879 seconds\n",
      "propagation took 0.5056421756744385 seconds\n",
      "propagation took 0.5074775218963623 seconds\n",
      "propagation took 0.5091574192047119 seconds\n",
      "propagation took 0.5425679683685303 seconds\n",
      "propagation took 0.510981559753418 seconds\n",
      "propagation took 0.5025079250335693 seconds\n",
      "propagation took 0.5020570755004883 seconds\n",
      "propagation took 0.5369856357574463 seconds\n",
      "propagation took 0.5179247856140137 seconds\n",
      "propagation took 0.49688243865966797 seconds\n",
      "propagation took 0.522284746170044 seconds\n",
      "propagation took 0.5010316371917725 seconds\n",
      "propagation took 0.6176068782806396 seconds\n",
      "propagation took 0.5164871215820312 seconds\n",
      "propagation took 0.5153448581695557 seconds\n",
      "propagation took 0.4960355758666992 seconds\n",
      "propagation took 0.5075588226318359 seconds\n",
      "propagation took 0.5249643325805664 seconds\n",
      "propagation took 0.5085639953613281 seconds\n",
      "propagation took 0.48514366149902344 seconds\n",
      "propagation took 0.5269014835357666 seconds\n",
      "propagation took 0.5066306591033936 seconds\n",
      "propagation took 0.5330626964569092 seconds\n",
      "propagation took 0.5229470729827881 seconds\n",
      "propagation took 0.5241641998291016 seconds\n",
      "propagation took 0.5033807754516602 seconds\n",
      "propagation took 0.5096127986907959 seconds\n",
      "propagation took 0.49190330505371094 seconds\n",
      "propagation took 0.5280008316040039 seconds\n",
      "propagation took 0.5209763050079346 seconds\n",
      "propagation took 0.5159773826599121 seconds\n",
      "propagation took 0.5520453453063965 seconds\n",
      "propagation took 0.49393391609191895 seconds\n",
      "propagation took 0.5227646827697754 seconds\n",
      "propagation took 0.5139234066009521 seconds\n",
      "propagation took 0.5240819454193115 seconds\n",
      "propagation took 0.5359303951263428 seconds\n",
      "propagation took 0.5323281288146973 seconds\n",
      "propagation took 0.5130517482757568 seconds\n",
      "propagation took 0.4994692802429199 seconds\n",
      "propagation took 0.5678925514221191 seconds\n",
      "propagation took 0.48177289962768555 seconds\n",
      "propagation took 0.5366635322570801 seconds\n",
      "propagation took 0.5072238445281982 seconds\n",
      "propagation took 0.5569155216217041 seconds\n",
      "propagation took 0.49715638160705566 seconds\n",
      "propagation took 0.5539288520812988 seconds\n",
      "propagation took 0.5275864601135254 seconds\n",
      "propagation took 0.530695915222168 seconds\n",
      "propagation took 0.5610156059265137 seconds\n",
      "propagation took 0.6139991283416748 seconds\n",
      "propagation took 0.507697343826294 seconds\n",
      "propagation took 0.533968448638916 seconds\n",
      "propagation took 0.5502622127532959 seconds\n",
      "propagation took 0.5321061611175537 seconds\n",
      "propagation took 0.519834041595459 seconds\n",
      "propagation took 0.4875307083129883 seconds\n",
      "propagation took 0.5171022415161133 seconds\n",
      "propagation took 0.5266444683074951 seconds\n",
      "propagation took 0.5152695178985596 seconds\n",
      "propagation took 0.532085657119751 seconds\n",
      "propagation took 0.5310213565826416 seconds\n",
      "propagation took 0.5419440269470215 seconds\n",
      "propagation took 0.5023138523101807 seconds\n",
      "propagation took 0.530566930770874 seconds\n",
      "propagation took 0.5379993915557861 seconds\n",
      "propagation took 0.5292983055114746 seconds\n",
      "propagation took 0.5160958766937256 seconds\n",
      "propagation took 0.5384838581085205 seconds\n",
      "propagation took 0.4990110397338867 seconds\n",
      "propagation took 0.514707088470459 seconds\n",
      "propagation took 0.6140804290771484 seconds\n",
      "propagation took 0.5380759239196777 seconds\n",
      "propagation took 0.5143153667449951 seconds\n",
      "propagation took 0.5479347705841064 seconds\n",
      "propagation took 0.5087056159973145 seconds\n",
      "propagation took 0.581489086151123 seconds\n",
      "propagation took 0.4959986209869385 seconds\n",
      "propagation took 0.5155775547027588 seconds\n",
      "propagation took 0.4929924011230469 seconds\n",
      "propagation took 0.5088913440704346 seconds\n",
      "propagation took 0.5102200508117676 seconds\n",
      "propagation took 0.5179786682128906 seconds\n",
      "propagation took 0.5202207565307617 seconds\n",
      "propagation took 0.5154986381530762 seconds\n",
      "propagation took 0.5263857841491699 seconds\n",
      "propagation took 0.5314950942993164 seconds\n",
      "propagation took 0.5107920169830322 seconds\n",
      "propagation took 0.521228551864624 seconds\n",
      "propagation took 0.512702465057373 seconds\n",
      "propagation took 0.5067753791809082 seconds\n",
      "propagation took 0.5382285118103027 seconds\n",
      "propagation took 0.5171821117401123 seconds\n",
      "propagation took 0.49211907386779785 seconds\n",
      "propagation took 0.5230140686035156 seconds\n",
      "propagation took 0.5539290904998779 seconds\n",
      "propagation took 0.5228569507598877 seconds\n",
      "propagation took 0.520988941192627 seconds\n",
      "propagation took 0.5123281478881836 seconds\n",
      "propagation took 0.5092730522155762 seconds\n",
      "propagation took 0.5125827789306641 seconds\n",
      "propagation took 0.5007896423339844 seconds\n",
      "propagation took 0.5430126190185547 seconds\n",
      "propagation took 0.5660607814788818 seconds\n",
      "propagation took 0.5091798305511475 seconds\n",
      "propagation took 0.5251674652099609 seconds\n",
      "propagation took 0.4675569534301758 seconds\n",
      "propagation took 0.5226278305053711 seconds\n",
      "propagation took 0.5052413940429688 seconds\n",
      "propagation took 0.5098912715911865 seconds\n",
      "propagation took 0.48703551292419434 seconds\n",
      "propagation took 0.5109343528747559 seconds\n",
      "propagation took 0.5130612850189209 seconds\n",
      "propagation took 0.5150649547576904 seconds\n",
      "propagation took 0.5440261363983154 seconds\n",
      "propagation took 0.5105171203613281 seconds\n",
      "propagation took 0.536907434463501 seconds\n",
      "propagation took 0.5007493495941162 seconds\n",
      "propagation took 0.5179848670959473 seconds\n",
      "propagation took 0.5509617328643799 seconds\n",
      "propagation took 0.5063059329986572 seconds\n",
      "propagation took 0.505357027053833 seconds\n",
      "propagation took 0.5130622386932373 seconds\n",
      "propagation took 0.5091147422790527 seconds\n",
      "propagation took 0.4985084533691406 seconds\n",
      "propagation took 0.5169837474822998 seconds\n",
      "propagation took 0.5129790306091309 seconds\n",
      "propagation took 0.4889862537384033 seconds\n",
      "propagation took 0.5676178932189941 seconds\n",
      "propagation took 0.5231478214263916 seconds\n",
      "propagation took 0.5314903259277344 seconds\n",
      "propagation took 0.5153286457061768 seconds\n",
      "propagation took 0.5248894691467285 seconds\n",
      "propagation took 0.5385756492614746 seconds\n",
      "propagation took 0.5278692245483398 seconds\n",
      "propagation took 0.5109851360321045 seconds\n",
      "propagation took 0.5251991748809814 seconds\n",
      "propagation took 0.5410058498382568 seconds\n",
      "propagation took 0.5766332149505615 seconds\n",
      "propagation took 0.5255646705627441 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(dna)):\n",
    "    inputs = tokenizer(dna[i], return_tensors = 'pt')[\"input_ids\"]\n",
    "    outputs : torch.Tensor = model(inputs.to(device))\n",
    "    logits = outputs[0]\n",
    "    hidden_states = outputs.hidden_states\n",
    "    checkpoint_vals, out_adj_list, topo_exec_order, fcn_map = lrp_engine(hidden_states.half(), out_adj_list, topo_exec_order, fcn_map)\n",
    "\n",
    "    if agg_checkpoint_vals is None:\n",
    "        agg_checkpoint_vals = checkpoint_vals\n",
    "    else:\n",
    "        for j in range(len(agg_checkpoint_vals)):\n",
    "            accumulate_variable_matrix(agg_checkpoint_vals[j], checkpoint_vals[j])\n",
    "            # agg_checkpoint_vals[j] += checkpoint_vals[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9035a394-36e7-4190-aa62-731229c1cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_backward_promise import AddBackwardPromise\n",
    "visited = sorted(list(visited1), key=lambda x: x._sequence_nr())\n",
    "visited_map = { str(node) : node for node in visited }\n",
    "promises = AddBackwardPromise.all_promises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb0bae-c09a-49e4-9c5e-3b7842d5ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relevance of attention mechanism\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set this to which layer you want to see\n",
    "layer_ind = 0\n",
    "\n",
    "# Use pooling with kernel size and stride set to d_head = d_model / num_heads = 64 (for DNABERT2)\n",
    "max_pool = torch.nn.MaxPool1d(kernel_size=64, stride=64)\n",
    "min_pool = lambda x: -max_pool(-x)\n",
    "avg_pool = torch.nn.AvgPool1d(kernel_size=64, stride=64)\n",
    "sum_pool = torch.nn.LPPool1d(kernel_size=64, stride=64, norm_type=1)\n",
    "\n",
    "pools = {\n",
    "    \"Max Pooling\": max_pool,\n",
    "    \"Min Pooling\": min_pool,\n",
    "    \"Average Pooling\": avg_pool,\n",
    "    \"Sum Pooling\": sum_pool,\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots((len(pools) + 1) // 2, 2, figsize=(10,8))\n",
    "fig.subplots_adjust(top=1.0)\n",
    "\n",
    "for i, (pool_name, pool_layer) in enumerate(list(pools.items())):\n",
    "    # Make a heatmap for each type of pooling\n",
    "    checkpoint_output = pool_layer(torch.unsqueeze(agg_checkpoint_vals[layer_ind], 0))\n",
    "    lrp_shape = checkpoint_output.shape\n",
    "    sns.heatmap(checkpoint_output.view((lrp_shape[-2], lrp_shape[-1])).cpu(), ax=axs[i // 2, i % 2])\n",
    "    axs[i // 2, i % 2].set_title(pool_name)\n",
    "    axs[i // 2, i % 2].set_xlabel(\"head index\")\n",
    "    axs[i // 2, i % 2].set_ylabel(\"token index\")\n",
    "\n",
    "# Remove overflow subplot\n",
    "if i < ((len(pools) + 1) // 2) * 2 - 1:\n",
    "    axs[i // 2, 1].set_axis_off()\n",
    "fig.suptitle(f\"Post-Attention Relevance at layer {layer_ind}, pooled by head.\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5493ab1-c505-4e20-bf0f-dbf7bf4871b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory (bytes): 8015348736\n"
     ]
    }
   ],
   "source": [
    "peak = torch.cuda.max_memory_allocated()\n",
    "print(\"Peak memory (bytes):\", peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe67d55-bde6-4336-b267-8eb1b854208b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
