{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d4b71f-983e-4e46-8ab9-149660470cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched GELU\n",
      "Patched LayerNorm\n",
      "Patched MultiheadAttention\n",
      "Patched Zennit BasicHook's forward\n",
      "Patched Zennit BasicHook's backward\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification\n",
    "from torchvision.models import vision_transformer\n",
    "\n",
    "from zennit.image import imgify\n",
    "from zennit.composites import LayerMapComposite\n",
    "import zennit.rules as z_rules\n",
    "\n",
    "module_path = os.path.join(os.getcwd(), '.')\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from lxt.efficient import monkey_patch, monkey_patch_zennit\n",
    "\n",
    "# Modify the Vision Transformer module to compute Layer-wise Relevance Propagation (LRP)\n",
    "# in the backward pass. For ViTs, we utilize the LRP Gamma rule. It is implemented\n",
    "# inside the 'zennit' library. To make it compatible with LXT, we also monkey patch it. That's it.\n",
    "monkey_patch(ViTForImageClassification, verbose=True)\n",
    "monkey_patch_zennit(verbose=True)\n",
    "\n",
    "\n",
    "def get_vit_imagenet(device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Vision Transformer (ViT) model with ImageNet weights.\n",
    "    \n",
    "    Parameters:\n",
    "    device (str): Device to load the model on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, weights) - The ViT model and its pre-trained weights\n",
    "    \"\"\"\n",
    "    weights =vision_transformer.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "    model = vision_transformer.vit_b_16(weights=weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Deactivate gradients on parameters to save memory\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "        \n",
    "    return model, weights\n",
    "\n",
    "# Load a couple of dog and cat images from ImageNet (torchvision)\n",
    "def get_imagenet_samples(num_samples=2):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # Download ImageNet val subset (tiny-imagenet or use CIFAR10 for demo)\n",
    "    # TODO: Mybe we should use a Hi Res dataset instead for the examples\n",
    "    dataset = datasets.CIFAR10(root='../../src/experiments/data', train=False, download=True, transform=transform)\n",
    "    # Class 3 = cat, Class 5 = dog\n",
    "    imgs_list = []\n",
    "    labels_list = []\n",
    "    for image, label in dataset:\n",
    "        imgs_list.append(image)\n",
    "        labels_list.append(label)\n",
    "        if len(imgs_list) >= num_samples:\n",
    "            break\n",
    "    return imgs_list, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919a869f-357d-483e-9e30-6ec5acd26b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ViT model\n",
    "# model, weights = get_vit_imagenet()\n",
    "model = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\n",
    "model.to(\"cuda\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "imgs_list, labels_list = get_imagenet_samples(1000)\n",
    "\n",
    "# Store the generated heatmaps\n",
    "heatmaps = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5f1927-4fc6-4598-8ee9-6d0dd58370c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:46<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different gamma values for Conv2d and Linear layers\n",
    "# Gamma is a hyperparameter in LRP that controls how much positive vs. negative\n",
    "# contributions are considered in the explanation\n",
    "# for conv_gamma, lin_gamma in itertools.product([0.1, 0.25, 100], [0, 0.01, 0.05, 0.1, 1]):\n",
    "conv_gamma = 100\n",
    "lin_gamma = 1\n",
    "for i in tqdm(range(len(imgs_list))):\n",
    "    input_tensor = imgs_list[i].unsqueeze(0).to(\"cuda\")\n",
    "    # input_tensor = weights.transforms()(image).unsqueeze(0).to(\"cuda\")\n",
    "    input_tensor.grad = None  # Reset gradients\n",
    "    # print(\"Gamma Conv2d:\", conv_gamma, \"Gamma Linear:\", lin_gamma)\n",
    "\n",
    "    # Define rules for the Conv2d and Linear layers using 'zennit'\n",
    "    # LayerMapComposite maps specific layer types to specific LRP rule implementations\n",
    "    zennit_comp = LayerMapComposite([\n",
    "        (torch.nn.Conv2d, z_rules.Gamma(conv_gamma)),\n",
    "        (torch.nn.Linear, z_rules.Gamma(lin_gamma)),\n",
    "    ])\n",
    "\n",
    "    # Register the composite rules with the model\n",
    "    zennit_comp.register(model)\n",
    "\n",
    "    # Forward pass with gradient tracking enabled\n",
    "    y = model(input_tensor.requires_grad_()).logits\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    _, top5_classes = torch.topk(y, 5, dim=1)\n",
    "    top5_classes = top5_classes.squeeze(0).tolist()\n",
    "\n",
    "    # Get the class labels\n",
    "    labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    # labels = weights.meta[\"categories\"]\n",
    "    top5_labels = [labels[class_idx] for class_idx in top5_classes]\n",
    "\n",
    "    # Print the top 5 predictions and their labels\n",
    "    # for i, class_idx in enumerate(top5_classes):\n",
    "    #     print(f'Top {i+1} predicted class: {class_idx}, label: {top5_labels[i]}')\n",
    "\n",
    "    # Backward pass for the highest probability class\n",
    "    # This initiates the LRP computation through the network\n",
    "    y[0, top5_classes[0]].backward()\n",
    "\n",
    "    # Remove the registered composite to prevent interference in future iterations\n",
    "    zennit_comp.remove()\n",
    "\n",
    "    # Calculate the relevance by computing Gradient * Input\n",
    "    # This is the final step of LRP to get the pixel-wise explanation\n",
    "    heatmap = (input_tensor * input_tensor.grad).sum(1)\n",
    "\n",
    "    # Normalize relevance between [-1, 1] for plotting\n",
    "    heatmap = heatmap / abs(heatmap).max()\n",
    "\n",
    "    # Store the normalized heatmap\n",
    "    heatmaps.append(heatmap[0].detach().clone().cpu())\n",
    "\n",
    "# torch.save(torch.stack(heatmaps), \"zennit_lrp_attrs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b25fb8-3bcf-4367-a0e0-3f52317edc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.stack(heatmaps), \"attnlrp_cifar10_attrs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e434623-2f0e-490d-a7c5-eca32087e7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
