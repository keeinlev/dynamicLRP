{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c1e774-78ed-4966-a00c-4ea2b12d30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95de822-b524-4216-94cc-01a98d1361c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "run_our_lrp = True\n",
    "attn_eager = False\n",
    "use_bf16 = True\n",
    "if use_bf16:\n",
    "    lrp_dtype = torch.bfloat16\n",
    "    dtype_name = \"bf16\"\n",
    "else:\n",
    "    lrp_dtype = torch.float32\n",
    "    dtype_name = \"f32\"\n",
    "if run_our_lrp:\n",
    "    lrp_name = \"our\"\n",
    "else:\n",
    "    lrp_name = \"attn\"\n",
    "if attn_eager:\n",
    "    attn_mode = \"eager\"\n",
    "else:\n",
    "    attn_mode = \"sdpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959c5669-6748-4a90-a2df-916385342e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not run_our_lrp:\n",
    "    from lxt.explicit.models.llama import LlamaForCausalLM, attnlrp\n",
    "    from lxt.utils import pdf_heatmap, clean_tokens\n",
    "    import transformers\n",
    "else:\n",
    "    from transformers import LlamaForCausalLM\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from lrp_engine import LRPEngine\n",
    "\n",
    "path = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path, torch_dtype=lrp_dtype, device_map=\"cuda\", attn_implementation=attn_mode)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "if not run_our_lrp:\n",
    "    # apply AttnLRP rules\n",
    "    attnlrp.register(model)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d6e00-d269-4364-a344-aee61b7acb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llama(model, tokenizer, input_ids, lrp=None):\n",
    "    # get input embeddings so that we can compute gradients w.r.t. input embeddings\n",
    "    input_embeds = model.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    # inference and get the maximum logit at the last position\n",
    "    output_logits = model(inputs_embeds=input_embeds.requires_grad_()).logits\n",
    "    max_logits, max_indices = torch.max(output_logits[0, -1, :], dim=-1)\n",
    "    \n",
    "    # get the top k tokens and their logits\n",
    "    topk_logits, topk_indices = torch.topk(output_logits[0, -1, :], k=5, dim=-1)\n",
    "    \n",
    "    # convert token indices to strings\n",
    "    topk_tokens = tokenizer.convert_ids_to_tokens(topk_indices)\n",
    "    \n",
    "    # initialize relevance scores with max_logits itself and backpropagate\n",
    "    if lrp is not None:\n",
    "        relevance = lrp.run(max_logits.unsqueeze(0))\n",
    "        if len(relevance) > 2:\n",
    "            print(\"error\")\n",
    "            return relevance, 1\n",
    "        relevance = relevance[1][0][0].float().cpu()\n",
    "    else:\n",
    "        max_logits.backward(max_logits)\n",
    "        relevance = input_embeds.grad.float().sum(-1).cpu()[0] # cast to float32 before summation for higher precision\n",
    "    \n",
    "    # normalize relevance between [-1, 1] for plotting\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "\n",
    "    # print(relevance.topk(5))\n",
    "\n",
    "    topk_indices = topk_indices.tolist()\n",
    "    non_start_end_tok_relevance = relevance\n",
    "    non_start_end_tok_relevance[0] = 0.0\n",
    "    non_start_end_tok_relevance[-1] = 0.0\n",
    "    topk_attr_inds = input_ids[0][non_start_end_tok_relevance.topk(5).indices].tolist()\n",
    "    top_attr_ind = topk_attr_inds[0]\n",
    "    union = len(set(topk_attr_inds).union(set(topk_indices)))\n",
    "    intersect = len(set(topk_attr_inds).intersection(set(topk_indices)))\n",
    "\n",
    "    # print(topk_attr_inds, topk_indices)\n",
    "    # print(tokenizer.convert_ids_to_tokens(topk_attr_inds), tokenizer.convert_ids_to_tokens(topk_indices))\n",
    "\n",
    "    top1_hit = top_attr_ind == topk_indices[0]\n",
    "\n",
    "    return top1_hit, relevance#, union, intersect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcbedba-8a05-40fe-92a9-68c1be21ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1580e9d1-3804-409e-ae09-e1d30f07f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_our_lrp:\n",
    "    lrp = LRPEngine(use_attn_lrp=True, dtype=lrp_dtype)\n",
    "else:\n",
    "    lrp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca5c21a-3ef3-426e-b93c-087deacd8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [ example for example in dataset[\"validation\"] if example[\"answers\"][\"text\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a548e6d-f569-47f5-b7d2-eac6ed30ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[0]\n",
    "question = example[\"question\"]\n",
    "context = example[\"context\"]\n",
    "prompt = context + \" \" + question\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ab2731-5c42-4af1-967c-4d3a8fe3937a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Warmup iteration\n",
    "info, _ = evaluate_llama(model, tokenizer, input_ids, lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "374bed5a-a583-44ac-8d46-58edc5a27959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lrp.promise_bucket.start_nodes_to_promise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a3101b-53e4-4dc5-b0bb-169aa2a19d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5928 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids.shape[-\u001b[32m1\u001b[39m] > \u001b[32m512\u001b[39m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m top1_hit, relevance = \u001b[43mevaluate_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(answers)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m top1_hit:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mevaluate_llama\u001b[39m\u001b[34m(model, tokenizer, input_ids, lrp)\u001b[39m\n\u001b[32m     33\u001b[39m non_start_end_tok_relevance[\u001b[32m0\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m     34\u001b[39m non_start_end_tok_relevance[-\u001b[32m1\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m topk_attr_inds = \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_start_end_tok_relevance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m top_attr_ind = topk_attr_inds[\u001b[32m0\u001b[39m]\n\u001b[32m     37\u001b[39m union = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(topk_attr_inds).union(\u001b[38;5;28mset\u001b[39m(topk_indices)))\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "top1_label_hits = 0\n",
    "top1_model_hits = 0\n",
    "total_examples = 0\n",
    "total_intersect = 0\n",
    "total_union = 0\n",
    "\n",
    "for example in tqdm(ds):\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = example[\"answers\"][\"text\"]\n",
    "\n",
    "    prompt = context + \" \" + question\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)\n",
    "    if input_ids.shape[-1] > 512:\n",
    "        continue\n",
    "    \n",
    "    top1_hit, relevance = evaluate_llama(model, tokenizer, input_ids, lrp)\n",
    "    # print(answers)\n",
    "    if top1_hit:\n",
    "        top1_model_hits += 1\n",
    "    total_examples += 1\n",
    "\n",
    "    results.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attr\": relevance.detach().cpu(),\n",
    "    })\n",
    "    \n",
    "    if not (total_examples % 100):\n",
    "        print(top1_model_hits, total_examples, top1_model_hits / total_examples)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61eb19-48c1-4386-9e95-4bfded7c3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxt.utils import pdf_heatmap, clean_tokens\n",
    "os.chdir(\"heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2766fc-cda6-4d7c-bf8b-04f00865bcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n",
      "PDF file generated successfully.\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u0130' in position 938: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m tokens = tokenizer.convert_ids_to_tokens(result[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      5\u001b[39m tokens = clean_tokens(tokens)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mpdf_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlrp_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mlrp_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdtype_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattn_mode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_heatmap_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_2.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxelatex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# pdf_heatmap(tokens, relevance, path=f'{lrp_name}lrp_single_{attn_mode}_test2.pdf', backend='xelatex')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Programming\\research\\lrp\\external\\LRP-eXplains-Transformers\\lxt\\utils.py:92\u001b[39m, in \u001b[36mpdf_heatmap\u001b[39m\u001b[34m(words, relevances, cmap, path, delete_aux_files, backend)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m relevances.min() >= -\u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m relevances.max() <= \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe relevances must be normalized between -1 and 1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m latex_code = _generate_latex(words, relevances, cmap=cmap)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43m_compile_latex_to_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatex_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_aux_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelete_aux_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Programming\\research\\lrp\\external\\LRP-eXplains-Transformers\\lxt\\utils.py:53\u001b[39m, in \u001b[36m_compile_latex_to_pdf\u001b[39m\u001b[34m(latex_code, path, delete_aux_files, backend)\u001b[39m\n\u001b[32m     50\u001b[39m os.makedirs(path.parent, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path.with_suffix(\u001b[33m\"\u001b[39m\u001b[33m.tex\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     f.write(latex_code)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Use pdflatex to generate PDF file\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m'\u001b[39m\u001b[33mpdflatex\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.charmap_encode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors,encoding_table)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\u0130' in position 938: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results):\n",
    "    relevance = result[\"attr\"]\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(result[\"input_ids\"][0])\n",
    "    tokens = clean_tokens(tokens)\n",
    "    pdf_heatmap(tokens, relevance, path=f'{lrp_name}lrp_{dtype_name}_{attn_mode}_heatmap_{i}_2.pdf', backend='xelatex')\n",
    "    # pdf_heatmap(tokens, relevance, path=f'{lrp_name}lrp_single_{attn_mode}_test2.pdf', backend='xelatex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dd308-51fa-4c4b-bc3f-feb38369fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top1_model_hits / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c714d1-578e-48e6-8265-abfb1b016b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
